{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f01831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd28134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967894ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3080f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(*input_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.pi = nn.Linear(fc2_dims, n_actions)\n",
    "        self.v = nn.Linear(fc2_dims, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pi = self.pi(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        return (pi, v)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions, \n",
    "                 gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.actor_critic = ActorCriticNetwork(lr, input_dims, n_actions, \n",
    "                                               fc1_dims, fc2_dims)\n",
    "        self.log_prob = None\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor_critic.device)\n",
    "        probabilities, _ = self.actor_critic.forward(state)\n",
    "        probabilities = F.softmax(probabilities, dim=1)\n",
    "        action_probs = T.distributions.Categorical(probabilities)\n",
    "        action = action_probs.sample()\n",
    "        log_prob = action_probs.log_prob(action)\n",
    "        self.log_prob = log_prob\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def learn(self, state, reward, state_, done):\n",
    "        self.actor_critic.optimizer.zero_grad()\n",
    "\n",
    "#         state = T.tensor([state], dtype=T.float).to(self.actor_critic.device)\n",
    "#         state_ = T.tensor([state_], dtype=T.float).to(self.actor_critic.device)\n",
    "#         reward = T.tensor(reward, dtype=T.float).to(self.actor_critic.device)\n",
    "\n",
    "        state = T.tensor(np.array([state]), dtype=T.float).to(self.actor_critic.device)\n",
    "        state_ = T.tensor(np.array([state_]), dtype=T.float).to(self.actor_critic.device)\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.actor_critic.device)\n",
    "\n",
    "        _, critic_value = self.actor_critic.forward(state)\n",
    "        _, critic_value_ = self.actor_critic.forward(state_)\n",
    "\n",
    "        delta = reward + self.gamma*critic_value_*(1-int(done)) - critic_value\n",
    "\n",
    "        actor_loss = -self.log_prob*delta\n",
    "        critic_loss = delta**2\n",
    "\n",
    "        (actor_loss + critic_loss).backward()\n",
    "        self.actor_critic.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58338e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -89597.7 average score -89597.7\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "num_features = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "agent = Agent(gamma=0.99, lr=5e-6, input_dims=[num_features], n_actions=num_actions,\n",
    "              fc1_dims=2048, fc2_dims=1536)\n",
    "n_games = 3000\n",
    "\n",
    "fname = 'ACTOR_CRITIC_' + 'lunar_lander_' + str(agent.fc1_dims) + \\\n",
    "        '_fc1_dims_' + str(agent.fc2_dims) + '_fc2_dims_lr' + str(agent.lr) +\\\n",
    "        '_' + str(n_games) + 'games'\n",
    "figure_file = 'plots/' + fname + '.png'\n",
    "\n",
    "scores = []\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    observation = env.reset()[0]\n",
    "    score = 0\n",
    "    iter = 0\n",
    "    while not done and iter < 5000:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, trunc, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.learn(observation, reward, observation_, done)\n",
    "        observation = observation_\n",
    "        iter += 1\n",
    "    scores.append(score)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print('episode ', i, 'score %.1f' % score,\n",
    "                'average score %.1f' % avg_score)\n",
    "\n",
    "x = [i+1 for i in range(n_games)]\n",
    "plot_learning_curve(x, scores, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91952177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69202f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96350980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce67ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximates pi(a | s)\n",
    "class PolicyModelTorch(torch.nn.Module):\n",
    "  def __init__(self, D, K, hidden_layer_sizes):\n",
    "    super().__init__()\n",
    "    self.nn_model = torch.nn.Sequential()\n",
    "    prev_dim = D\n",
    "    for layer_num, layer_size in enumerate(hidden_layer_sizes):\n",
    "        self.nn_model.add_module(f\"fc_{layer_num}\", torch.nn.Linear(prev_dim, layer_size))\n",
    "        self.nn_model.add_module(f\"act_{layer_num}\", torch.nn.ReLU())\n",
    "        prev_dim = layer_size\n",
    "    self.nn_model.add_module(\"final_hidden_layer\", torch.nn.Linear(prev_dim, K))\n",
    "    self.nn_model.add_module(\"softmax\", torch.nn.Softmax())\n",
    "\n",
    "  def predict(self, X):\n",
    "    return self.nn_model(X)\n",
    "\n",
    "  def sample_action(self, X):\n",
    "# return self.predict(X)\n",
    "    with torch.no_grad():\n",
    "        p = self.predict(X).numpy()\n",
    "#         print(\"in sample action predict: \", p)\n",
    "#         p /= p.sum()\n",
    "       # return np.random.multinomial(len(p), p)\n",
    "        p[-1] = 1 - np.sum(p[0:-1])\n",
    "        return np.random.choice(len(p), p=p)\n",
    "\n",
    "\n",
    "def policy_cost_function(preds, actions, advantages, K):\n",
    "    # actions is a bunch of indices of (num_examples,) K is num_actions.\n",
    "    # preds should be (num_examples, num_actions)\n",
    "    # advantages are (num_examples,)\n",
    "    # (num_examples, num_actions)\n",
    "#     print(\"preds: \", preds)\n",
    "    pred_action_scores = preds * torch.nn.functional.one_hot(actions, K)\n",
    "#     print(\"pred_action_scores: \", pred_action_scores)\n",
    "    # (num_examples,)\n",
    "    selected_probs = torch.log(torch.sum(pred_action_scores, 1))\n",
    "#     print(\"selected_probs: \", selected_probs)\n",
    "    cost = -torch.sum(advantages * selected_probs)\n",
    "    #cost = torch.sum(advantages * selected_probs)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de80b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximates V(s)\n",
    "class ValueModelTorch(torch.nn.Module):\n",
    "  def __init__(self, D, hidden_layer_sizes):\n",
    "    super().__init__()\n",
    "    self.nn_model = torch.nn.Sequential()\n",
    "    M1 = D\n",
    "    for layer_num, M2 in enumerate(hidden_layer_sizes):\n",
    "        self.nn_model.add_module(f\"fc_{layer_num}\", torch.nn.Linear(M1, M2))\n",
    "#         self.nn_model.add_module(f\"act_{layer_num}\", torch.nn.Tanh())\n",
    "        self.nn_model.add_module(f\"act_{layer_num}\", torch.nn.ReLU())\n",
    "        M1 = M2\n",
    "    self.nn_model.add_module(\"final_hidden_layer\", torch.nn.Linear(M1, 1))\n",
    "      \n",
    "    #self.cost = torch.nn.MSELoss()\n",
    "\n",
    "#     # calculate output and cost\n",
    "#     Z = self.X\n",
    "#     for layer in self.layers:\n",
    "#       Z = layer.forward(Z)\n",
    "#     Y_hat = tf.reshape(Z, [-1]) # the output\n",
    "#     self.predict_op = Y_hat\n",
    "\n",
    "#     cost = tf.reduce_sum(tf.square(self.Y - Y_hat))\n",
    "#     self.train_op = tf.train.GradientDescentOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "#   def partial_fit(self, X, Y):\n",
    "#     self.session.run(self.train_op, feed_dict={self.X: X, self.Y: Y})\n",
    "\n",
    "  def predict(self, X):\n",
    "    return self.nn_model(X)\n",
    "\n",
    "\n",
    "  def cost_function(self, preds, actuals):\n",
    "    return torch.sum((actuals - preds)**2)\n",
    "    #return self.cost(preds, actuals)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbfbedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_td(env, pmodel, vmodel, gamma, p_opt, v_opt):\n",
    "  observation = env.reset()[0]\n",
    "  done = False\n",
    "  totalreward = 0\n",
    "  iters = 0\n",
    "\n",
    "  while not done and iters < 2000:\n",
    "#   while not done and iters < 20:\n",
    "    # if we reach 2000, just quit, don't want this going forever\n",
    "    # the 200 limit seems a bit early\n",
    "    \n",
    "    \n",
    "    prev_observation = observation\n",
    "    prev_obs_tensor = torch.from_numpy(observation)\n",
    "    action = pmodel.sample_action(prev_obs_tensor)\n",
    "    observation, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "    # if done:\n",
    "    #   reward = -200\n",
    "\n",
    "    # update the models\n",
    "#     with torch.no_grad():\n",
    "    V_next = vmodel.predict(torch.from_numpy(observation))\n",
    "    G = reward + gamma*V_next\n",
    "    advantage = G - vmodel.predict(torch.from_numpy(prev_observation))\n",
    "#     pmodel.partial_fit(prev_observation, action, advantage)\n",
    "#     vmodel.partial_fit(prev_observation, G)\n",
    "#     adv_copy = torch.tensor(advantage.detach().numpy())\n",
    "    cost = policy_cost_function(pmodel.predict(torch.from_numpy(prev_observation)), \n",
    "                                torch.tensor([action]),\n",
    "                                advantage, K)\n",
    "    #print(cost, advantage)\n",
    "    cost.backward()\n",
    "    p_opt.step()\n",
    "    \n",
    "    G_copy = torch.tensor(G.detach().numpy())\n",
    "    vcost = vmodel.cost_function(vmodel.predict(torch.from_numpy(prev_observation)), \n",
    "                              G_copy)\n",
    "    vcost.backward()\n",
    "    v_opt.step()\n",
    "    p_opt.zero_grad()\n",
    "    v_opt.zero_grad()\n",
    "\n",
    "    if reward == 1: # if we changed the reward to -200\n",
    "      totalreward += reward\n",
    "    iters += 1\n",
    "\n",
    "  return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04951aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "D = env.observation_space.shape[0]\n",
    "K = env.action_space.n\n",
    "pmodel = PolicyModelTorch(D, K, [10, 5])\n",
    "vmodel = ValueModelTorch(D, [10])\n",
    "\n",
    "p_opt = torch.optim.Adagrad(pmodel.parameters(), lr=0.0001, lr_decay=0.99)\n",
    "v_opt = torch.optim.Adagrad(vmodel.parameters(), lr=0.0001, lr_decay=0.99)\n",
    "gamma = 0.99\n",
    "\n",
    "total_reward = play_one_td(env, pmodel, vmodel, gamma, p_opt, v_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9dcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
