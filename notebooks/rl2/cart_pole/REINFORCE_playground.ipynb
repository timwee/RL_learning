{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5d7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61304977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b06d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent():\n",
    "    def __init__(self, lr, input_dims, gamma=0.99, n_actions=4):\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.reward_memory = []\n",
    "        self.action_memory = []\n",
    "\n",
    "        self.policy = PolicyNetwork(self.lr, input_dims, n_actions)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "#         state = T.Tensor([observation]).to(self.policy.device)\n",
    "        state = T.Tensor(np.array([observation])).to(self.policy.device)\n",
    "        probabilities = F.softmax(self.policy.forward(state))\n",
    "        action_probs = T.distributions.Categorical(probabilities)\n",
    "        action = action_probs.sample()\n",
    "        log_probs = action_probs.log_prob(action)\n",
    "        self.action_memory.append(log_probs)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def store_rewards(self, reward):\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "    def learn(self):\n",
    "        self.policy.optimizer.zero_grad()\n",
    "\n",
    "        # G_t = R_t+1 + gamma * R_t+2 + gamma**2 * R_t+3\n",
    "        # G_t = sum from k=0 to k=T {gamma**k * R_t+k+1}\n",
    "        G = np.zeros_like(self.reward_memory, dtype=np.float64)\n",
    "        for t in range(len(self.reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(self.reward_memory)):\n",
    "                G_sum += self.reward_memory[k] * discount\n",
    "                discount *= self.gamma\n",
    "            G[t] = G_sum\n",
    "        G = T.tensor(G, dtype=T.float).to(self.policy.device)\n",
    "        \n",
    "        loss = 0\n",
    "        for g, logprob in zip(G, self.action_memory):\n",
    "            loss += -g * logprob\n",
    "        loss.backward()\n",
    "        self.policy.optimizer.step()\n",
    "\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9bb0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(scores, x, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d20526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jd/cghnyyt95l311hw30yl_42d80000gp/T/ipykernel_42366/346005588.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = F.softmax(self.policy.forward(state))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -107.84 average score -107.84\n",
      "episode  100 score -135.69 average score -181.55\n",
      "episode  200 score -182.17 average score -166.35\n",
      "episode  300 score -199.41 average score -169.22\n",
      "episode  400 score -90.66 average score -145.82\n",
      "episode  500 score -96.68 average score -123.89\n",
      "episode  600 score -179.54 average score -129.77\n",
      "episode  700 score -18.99 average score -125.17\n",
      "episode  800 score -89.31 average score -111.80\n",
      "episode  900 score -58.39 average score -119.64\n",
      "episode  1000 score -64.61 average score -107.15\n",
      "episode  1100 score -18.82 average score -97.08\n",
      "episode  1200 score -110.96 average score -83.71\n",
      "episode  1300 score -83.60 average score -88.67\n",
      "episode  1400 score -74.92 average score -56.74\n",
      "episode  1500 score -47.85 average score -42.03\n",
      "episode  1600 score -34.86 average score -32.74\n",
      "episode  1700 score 17.21 average score -17.84\n",
      "episode  1800 score -18.45 average score 10.75\n",
      "episode  1900 score -24.45 average score 58.24\n",
      "episode  2000 score 61.68 average score 72.05\n",
      "episode  2100 score 248.24 average score 89.09\n",
      "episode  2200 score 21.89 average score 84.73\n",
      "episode  2300 score 34.56 average score 23.56\n",
      "episode  2400 score -16.95 average score 20.34\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "n_games = 3000\n",
    "agent = PolicyGradientAgent(gamma=0.99, lr=0.0005, input_dims=[8],\n",
    "                            n_actions=4)\n",
    "\n",
    "fname = 'REINFORCE_' + 'lunar_lunar_lr' + str(agent.lr) + '_' \\\n",
    "        + str(n_games) + 'games'\n",
    "figure_file = 'plots/' + fname + '.png'\n",
    "\n",
    "scores = []\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    observation = env.reset()[0]\n",
    "    score = 0\n",
    "    iter = 0\n",
    "    while not done and iter < 5000:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, trunc, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_rewards(reward)\n",
    "        observation = observation_\n",
    "        iter += 1\n",
    "    agent.learn()\n",
    "    scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    if i % 100 == 0:\n",
    "        print('episode ', i, 'score %.2f' % score,\n",
    "                'average score %.2f' % avg_score)\n",
    "\n",
    "x = [i+1 for i in range(len(scores))]\n",
    "plot_learning_curve(scores, x, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51329046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
